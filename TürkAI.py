import streamlit as st
import requests
from bs4 import BeautifulSoup
import datetime
import sqlite3
import hashlib
import urllib.parse
import re
from fpdf import FPDF
import time

# --- âš™ï¸ SÄ°STEM AYARLARI ---
st.set_page_config(page_title="TÃ¼rkAI | Profesyonel Terminal", layout="wide")

# --- ğŸ› ï¸ GELÄ°ÅMÄ°Å METÄ°N TEMÄ°ZLEME MOTORU ---
def profesyonel_temizle(raw_text):
    if not raw_text: return ""
    
    # 1. Wikipedia ve Haber sitesi "Ã§Ã¶plerini" ayÄ±kla
    cop_kelimeler = [
        "Ä°Ã§eriÄŸe atla", "Ana menÃ¼yÃ¼ aÃ§", "Ara", "DeÄŸiÅŸtir", "KaynaÄŸÄ± deÄŸiÅŸtir", 
        "GiriÅŸ yap", "KayÄ±t ol", "Daha fazla bilgi", "Abone olmak iÃ§in tÄ±klayÄ±n",
        "Ana sayfa", "ManÅŸet haber", "SeÃ§tiklerimiz", "DiÄŸer haberler"
    ]
    for kelime in cop_kelimeler:
        raw_text = raw_text.replace(kelime, "")

    # 2. KÃ¶ÅŸeli parantezleri [1], [not 1] temizle
    raw_text = re.sub(r'\[.*?\]', '', raw_text)
    
    # 3. SatÄ±r baÅŸlarÄ±ndaki ve sonundaki boÅŸluklarÄ± buda, boÅŸ satÄ±rlarÄ± sil
    lines = [line.strip() for line in raw_text.splitlines() if line.strip()]
    
    # 4. BOÅLUK DÃœZELTME: Ã‡oklu boÅŸluklarÄ± ve satÄ±rlarÄ± teke indir
    clean_text = "\n\n".join(lines) # Paragraf arasÄ± Ã§ift satÄ±r
    clean_text = re.sub(r' +', ' ', clean_text) # Kelime arasÄ± tek boÅŸluk
    
    return clean_text

def tdk_temizle(json_data):
    try:
        res = []
        for madde in json_data:
            res.append(f"ğŸ“š {madde.get('madde', '').upper()}")
            for anl in madde.get('anlamlarListe', []):
                res.append(f"â€¢ {anl.get('anlam', '')}")
                if 'orneklerListe' in anl:
                    for o in anl['orneklerListe']:
                        res.append(f"  ğŸ‘‰ '{o.get('ornek')}'")
        return "\n".join(res)
    except: return "SÃ¶zlÃ¼k verisi ayrÄ±ÅŸtÄ±rÄ±lamadÄ±."

# --- ğŸŒ VERÄ° Ã‡EKME MOTORU ---
def veri_getir(url):
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) Brave/120.0.0.0'}
        response = requests.get(url, headers=headers, timeout=10)
        
        # TDK kontrolÃ¼
        if "sozluk.gov.tr" in url:
            return tdk_temizle(response.json())

        soup = BeautifulSoup(response.content, 'html.parser')
        
        # Gereksiz HTML etiketlerini tamamen kaldÄ±r
        for tag in ["script", "style", "nav", "footer", "header", "aside", "form", "button"]:
            for element in soup.find_all(tag):
                element.decompose()

        # Ana iÃ§eriÄŸe odaklan (Makale veya Ana GÃ¶vde)
        article = soup.find('article') or soup.find('main') or soup.body
        text = article.get_text(separator=' ')
        
        return profesyonel_temizle(text)
    except:
        return None

# --- ğŸ”‘ SESSION & GÄ°RÄ°Å ---
if "user" not in st.session_state: st.session_state.user = None

if not st.session_state.user:
    st.markdown("<h1 style='text-align:center;'>ğŸ‡¹ğŸ‡· TÃ¼rkAI GiriÅŸ</h1>", unsafe_allow_html=True)
    if st.button("Misafir Olarak BaÅŸla", use_container_width=True):
        st.session_state.user = "Misafir"
        st.rerun()
    st.stop()

# --- ğŸ–¥ï¸ ANA EKRAN ---
st.title("PROFESYONEL ARAÅTIRMA TERMÄ°NALÄ°")
st.markdown("> **Not:** Aramak istediÄŸiniz konunun **ANAHTAR KELÄ°MESÄ°NÄ°** yazÄ±nÄ±z. (Ã–rn: TÃ¼rkâœ…)")

col_arama, col_motor = st.columns([3, 1])
with col_arama:
    sorgu = st.text_input("AraÅŸtÄ±rma Konusu:", placeholder="Ã–rn: Sucuk, Fatih Sultan Mehmet...")
with col_motor:
    motor = st.selectbox("Motor", ["ğŸš€ Ansiklopedi (V1)", "ğŸ—ï¸ GÃ¼ndem/Haber (V2)"])

if st.button("ARAÅTIRMAYI BAÅLAT", type="primary", use_container_width=True):
    if sorgu:
        q = urllib.parse.quote(sorgu)
        st.session_state.sorgu_kelime = sorgu
        if "Ansiklopedi" in motor:
            st.session_state.kaynaklar = [
                f"https://sozluk.gov.tr/gts?ara={q}",
                f"https://tr.wikipedia.org/wiki/{q}",
                f"https://islamansiklopedisi.org.tr/ara?q={q}"
            ]
        else:
            st.session_state.kaynaklar = [
                f"https://www.bbc.com/turkce/search?q={q}",
                f"https://www.trthaber.com/haber/ara/?q={q}",
                f"https://www.ensonhaber.com/arama?q={q}"
            ]
        st.session_state.idx = 0
        st.session_state.arama_aktif = True

if st.session_state.get("arama_aktif"):
    kaynaklar = st.session_state.kaynaklar
    i = st.session_state.idx
    
    if i < len(kaynaklar):
        url = kaynaklar[i]
        st.info(f"ğŸ” Kaynak taranÄ±yor: {urllib.parse.urlparse(url).netloc}")
        
        sonuc = veri_getir(url)
        if sonuc:
            st.markdown(f"### ğŸ“„ {st.session_state.sorgu_kelime.upper()} - Analiz Raporu")
            st.markdown(f"<div style='background:#f0f2f6; color:#111; padding:20px; border-radius:10px; border-left:5px solid #800000; white-space: pre-wrap;'>{sonuc[:5000]}</div>", unsafe_allow_html=True)
            
            c1, c2 = st.columns(2)
            with c1:
                if st.button("ğŸ”„ Bu KaynaÄŸÄ± BeÄŸenmedim, SÄ±radakine GeÃ§", use_container_width=True):
                    st.session_state.idx += 1
                    st.rerun()
            with c2:
                st.success("Veri baÅŸarÄ±yla temizlendi ve optimize edildi.")
        else:
            st.session_state.idx += 1
            st.rerun()
    else:
        st.warning("TÃ¼m kaynaklar bitti. BaÅŸka bir anahtar kelime deneyebilirsin.")

# --- ğŸ“Š FOOTER ---
st.markdown("---")
st.markdown("<center><b>2026 Â© TÃ¼rkAI Profesyonel AraÅŸtÄ±rma Sistemi</b><br>BoÅŸluk Temizleme ModÃ¼lÃ¼: AKTÄ°F</center>", unsafe_allow_html=True)
