import streamlit as st
import requests
from bs4 import BeautifulSoup
import time

# Sayfa AyarlarÄ±
st.set_page_config(page_title="TÃ¼rkAI Pro - Analiz Sistemi", layout="wide")
st.title("ğŸ›¡ï¸ TÃ¼rkAI v1: Ã‡ok KanallÄ± Veri Analiz Motoru")
st.markdown("---")

# KullanÄ±cÄ± GiriÅŸi
konu = st.text_input("Analiz Edilecek Stratejik Konuyu Giriniz:", placeholder="Ã–rn: Kuantum Bilgisayarlar")

def veri_cek(url, headers):
    try:
        res = requests.get(url, headers=headers, timeout=10)
        if res.status_code == 200:
            return res.text
        return None
    except:
        return None

if st.button("DERÄ°N ANALÄ°ZÄ° BAÅLAT"):
    if konu:
        cols = st.columns(3)
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36'}
        
        # --- 1. KAPI: GOOGLE STRATEJÄ°K TARAMA ---
        with cols[0]:
            st.subheader("ğŸŒ Google KaynaklarÄ±")
            with st.spinner("Google kalkanÄ± zorlanÄ±yor..."):
                g_url = f"https://www.google.com/search?q={konu}+nedir+hakkÄ±nda+bilgi"
                g_data = veri_cek(g_url, headers)
                if g_data:
                    soup = BeautifulSoup(g_data, 'html.parser')
                    texts = [s.text for s in soup.find_all('span') if len(s.text) > 40]
                    if texts:
                        st.success("Veri Ã§ekildi.")
                        st.write(texts[0])
                    else:
                        st.error("Google eriÅŸimi kÄ±sÄ±tladÄ±.")
                else:
                    st.error("BaÄŸlantÄ± baÅŸarÄ±sÄ±z.")

        # --- 2. KAPI: DUCKDUCKGO (GÄ°ZLÄ° GEÃ‡Ä°T) ---
        with cols[1]:
            st.subheader("ğŸ¦† DuckDuckGo Analizi")
            with st.spinner("Alternatif yollar taranÄ±yor..."):
                d_url = f"https://duckduckgo.com/html/?q={konu}"
                d_data = veri_cek(d_url, headers)
                if d_data:
                    soup = BeautifulSoup(d_data, 'html.parser')
                    links = soup.find_all('a', class_='result__a')
                    if links:
                        st.success("Alternatif veri bulundu.")
                        st.write(links[0].text)
                    else:
                        st.warning("SonuÃ§ bulunamadÄ±.")
                else:
                    st.error("EriÅŸim engellendi.")

        # --- 3. KAPI: WIKIPEDIA (AKADEMÄ°K DOÄRULAMA) ---
        with cols[2]:
            st.subheader("ğŸ“š Akademik KayÄ±tlar")
            with st.spinner("ArÅŸivler inceleniyor..."):
                w_url = f"https://tr.wikipedia.org/wiki/{konu.replace(' ', '_')}"
                w_data = veri_cek(w_url, headers)
                if w_data:
                    soup = BeautifulSoup(w_data, 'html.parser')
                    p = soup.find_all('p')
                    if len(p) > 1:
                        st.success("Resmi kayÄ±tlar eÅŸleÅŸti.")
                        st.write(p[1].text[:500] + "...")
                    else:
                        st.warning("Wikipedia kaydÄ± bulunamadÄ±.")
                else:
                    st.error("ArÅŸiv baÄŸlantÄ±sÄ± koptu.")
    else:
        st.warning("LÃ¼tfen bir analiz konusu giriniz.")

st.markdown("---")
st.caption("TÃ¼rkAI v1 - GÃ¼venli ve Ã‡ok KanallÄ± Veri Ã‡ekme ProtokolÃ¼")
